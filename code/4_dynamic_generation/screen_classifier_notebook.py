# -*- coding: utf-8 -*-
"""screen_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at

"""

from google.colab import drive
drive.mount('/gdrive')

import json
import pandas as pd
from collections import Counter
import torch
import torch.utils.data as data
import os
import nltk
import numpy as np

text_files_path = "/gdrive/My Drive/OCR_text"

nltk.download('words')
words = set(nltk.corpus.words.words())

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)

def get_raw_text_data(image_name):
  with open(os.path.join(text_files_path, image_name), 'r', encoding='windows-1252') as text_file:
    raw_text_data = text_file.read().splitlines()
  return raw_text_data

def process_text_data(raw_data):
  remove_none_english = [w for w in raw_data if w in words]
  sentence = " ".join(remove_none_english) 
  return sentence

print(process_text_data(get_raw_text_data("aliexpress-signup-2-bbox-2453-screen.txt")))

test_app_name = "aliexpress"
embeddings_path = "/gdrive/My Drive/autoencoder_embeddings"

def generate_train_test_data():
    train_test_split_dict = {}
    idx = 0
    counter = 0
    with open(os.path.join(embeddings_path, "AllNamesOld.json")) as name_old_file:
        names_old_str = name_old_file.read()
        names_old = names_old_str.split(",")
        for name in names_old:
            name = name.split("/")[-1].split(".")[0]
            app_name = (name.split("-")[0]).lower()
            if app_name == test_app_name:
                train_test_split_dict[idx] = "test"
                counter += 1
            else:
                train_test_split_dict[idx] = "train"
            idx += 1
    
 

    with open(os.path.join(embeddings_path, "AllNames_augmented.json")) as name_augmented_file:
        names_augmented_str = name_augmented_file.read()
        names_augmented = names_augmented_str.split(",")
        for name in names_augmented:
            name = name.split("/")[-1][:-19]
            app_name = (name.split("-")[0]).lower()
            if app_name == test_app_name:
                train_test_split_dict[idx] = "test"
                counter += 1
            else:
                train_test_split_dict[idx] = "train"
            idx += 1

    print(counter)
    return train_test_split_dict

train_test_split_dict = generate_train_test_data()
print(train_test_split_dict)

def Merge(dict1, dict2):
    res = {**dict1, **dict2}
    return res

df1 = pd.read_csv("final_labels_all.csv", usecols=['screen', 'tag_screen'], na_filter=False)
df1.loc[df1["screen"].str.contains("/"), "screen"] = df1.screen.str.rsplit("/", n=1, expand=True)[1]
df1.screen = (df1.screen.str.rsplit(".", n=1, expand=True)[0])
label_dict1 = dict(zip(df1.screen, df1.tag_screen))

df2 = pd.read_csv("augmented_labels.csv", usecols=['screen', 'tag_screen'], na_filter=False)
label_dict2 = dict(zip(df2.screen, df2.tag_screen))

label_dict = Merge(label_dict1, label_dict2)

print(label_dict)

a = np.zeros(2)
b = np.array([5,6,7,8])
all = np.concatenate((a,b), axis = 0)
print(all)

embeddings_old = torch.load(os.path.join(embeddings_path, "AllEmbeddingsOld"), map_location='cpu')
print(embeddings_old.shape)
print(embeddings_old[0].shape)
print(embeddings_old[0])
embeddings_augmented = torch.load(os.path.join(embeddings_path, "AllEmbeddings_augmented"), map_location='cpu')
print(embeddings_augmented.shape)
print(embeddings_augmented[0].shape)
print(embeddings_augmented[0])

all_embeddings = np.concatenate((embeddings_old, embeddings_augmented), axis=0)
print(all_embeddings.shape)
print(all_embeddings[0])
print(all_embeddings[2138])
embedding_id_dict = {}

idx = 0
with open(os.path.join(embeddings_path, "AllNamesOld.json")) as name_old_file:
  names_old_str = name_old_file.read()
  names_old = names_old_str.split(",")
  for name in names_old:
    name = name.split("/")[-1].split(".")[0]
    embedding_id_dict[idx] = name
    idx += 1
with open(os.path.join(embeddings_path, "AllNames_augmented.json")) as name_augmented_file:
    names_augmented_str = name_augmented_file.read()
    names_augmented = names_augmented_str.split(",")
    for name in names_augmented:
        name = name.split("/")[-1][:-19]
        embedding_id_dict[idx] = name
        idx += 1

all_training_embeddings_lst = []
all_testing_embeddings_lst = []
training_idx = 0
testing_idx = 0
training_id_dict = {}
testing_id_dict = {}
for i in range(idx):
    if  train_test_split_dict[i] == "train":
        all_training_embeddings_lst.append(all_embeddings[i])
        training_id_dict[training_idx] = embedding_id_dict[i]
        training_idx += 1
    else:
        all_testing_embeddings_lst.append(all_embeddings[i])
        testing_id_dict[testing_idx] = embedding_id_dict[i]
        testing_idx += 1

all_training_embeddings_mat = np.vstack(all_training_embeddings_lst)
all_testing_embeddings_mat = np.vstack(all_testing_embeddings_lst)

print(all_training_embeddings_mat.shape)
print(all_testing_embeddings_mat.shape)
print(training_id_dict)
print(testing_id_dict)

class ScreenDataset(data.Dataset):
    def __init__(self, mode, labels_path):
        self.screens = {}

        df = pd.read_csv(labels_path, usecols=['tag_screen'], na_filter=False)
        self.screen_list = (df['tag_screen'].tolist())
        self.screen_cntr = Counter(self.screen_list)
        self.screen_dict = self.generate_screen_dict()
        print(self.screen_dict)

        embedding_mat = None
        name_id_dict = None
        if mode == "train":
          embedding_mat = all_training_embeddings_mat
          name_id_dict = training_id_dict
        elif mode == "test":
          embedding_mat = all_testing_embeddings_mat
          name_id_dict = testing_id_dict

        no_label = 0
        no_text = 0
        for id in name_id_dict.keys():
          self.screens[id] = {}
          self.screens[id]["layout_emb"] = embedding_mat[id,:]
          self.screens[id]["name"] = name_id_dict[id] 
          print(self.screens[id]["name"])
          try:
            self.screens[id]["text"] = process_text_data(get_raw_text_data(self.screens[id]["name"]+".txt"))
          except:
            self.screens[id]["text"] = ""
            no_text += 1
          try:
            self.screens[id]["screen_tag_id"] = self.screen_dict[label_dict[self.screens[id]["name"]]]
          except:
            no_label += 1
            self.screens[id]["screen_tag_id"] = 33

        
        print("no labels:" +str(no_label))
        print("no text file:" +str(no_text))

        self.ids = list(self.screens.keys())

    def generate_screen_dict(self):
        idx = 0
        s_dict = {}
        for tag in self.screen_cntr.keys():
            s_dict[tag] = idx
            idx += 1
        return s_dict

    def generate_widget_dict(self):
        idx = 0
        w_dict = {}
        for tag in self.widget_cntr.keys():
            w_dict[tag] = idx
            idx += 1
        return w_dict

    def __getitem__(self, index):
        screen = self.screens[index]
        return screen


    def __len__(self):
        return len(self.ids)


def get_loader(mode, batch_size, num_workers, labels_path):
    screen = ScreenDataset(mode, labels_path)
    data_loader = torch.utils.data.DataLoader(screen, batch_size=batch_size,
                                              num_workers=num_workers)
    return data_loader

import torch
import torch.nn as nn
import torchvision.models as models

class ScreenClassifier(nn.Module):
    def __init__(self, bert_size=768, layout_size=4608, output_size=34):
        super().__init__()
  
        self.bert_size = bert_size
        self.layout_size = layout_size

        self.layer_1 = nn.Linear(self.bert_size+self.layout_size, 512)
        self.layer_2 = nn.Linear(512, 256)
        self.layer_3 = nn.Linear(256, 128)
        self.layer_out = nn.Linear(128, output_size)

        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=0.2)
        self.batchnorm1 = nn.BatchNorm1d(512)
        self.batchnorm2 = nn.BatchNorm1d(256)
        self.batchnorm3 = nn.BatchNorm1d(128)

    def forward(self, text_emb, layout_emb):
        x = torch.cat((text_emb, layout_emb), 1).cuda()
        
        o1 = self.layer_1(x).cuda()
        o1 = self.batchnorm1(o1).cuda()
        o1 = self.relu(o1).cuda()

        o2 = self.layer_2(o1).cuda()
        o2 = self.batchnorm2(o2).cuda()
        o2 = self.relu(o2).cuda()
        o2 = self.dropout(o2).cuda()

        o3 = self.layer_3(o2).cuda()
        o3 = self.batchnorm3(o3).cuda()
        o3 = self.relu(o3).cuda()
        o3 = self.dropout(o3).cuda()

        o = self.layer_out(o3).cuda()
        return o

! pip install sentence_transformers
import torch.optim as optim
from sentence_transformers import SentenceTransformer

BATCH_SIZE = 100
LEARNING_RATE = 0.0007

train_loader = get_loader("train", BATCH_SIZE, 2, "final_labels_all.csv")
test_loader =  get_loader("test", BATCH_SIZE, 2, "final_labels_all.csv")

bert = SentenceTransformer('bert-base-nli-mean-tokens').to(device)

ui_screen_model = ScreenClassifier().to(device)
criterion = nn.CrossEntropyLoss().to(device)
optimizer = optim.Adam(ui_screen_model.parameters(), lr=LEARNING_RATE)

EPOCHS = 15

for epoch in range(EPOCHS):
  print(epoch)
  batch_cntr = 0
  acc_sum = 0
  for i, screen in enumerate(train_loader):
      text = torch.as_tensor(bert.encode(screen["text"]), device=device)
      layout_embedding = screen["layout_emb"].to(device)
      out = ui_screen_model(text, layout_embedding)
      y_pred = torch.log_softmax(out, dim=1)
      _, y_pred_tags = torch.max(y_pred, dim = 1) 
      tag_ids = screen["screen_tag_id"].to(device)
      correct_pred = (y_pred_tags == tag_ids).float()
      acc = correct_pred.sum() / len(correct_pred)
      acc_sum += acc
      batch_cntr += 1
      loss = criterion(y_pred, tag_ids)
      loss.backward()
      optimizer.step()
      optimizer.zero_grad()
  print("average epoch train acc:"+ str(acc_sum/batch_cntr))

  print(loss)

all_correct = 0
all_cnt = 0
correct_top_n = 0
all_correct_top_10 = 0
with torch.no_grad():
    ui_screen_model.eval()
    for i, screen in enumerate(test_loader):
        print(screen["name"])
        text = torch.as_tensor(bert.encode(screen["text"]), device=device)
        layout_embedding = screen["layout_emb"].to(device)
        out = ui_screen_model(text, layout_embedding)
        y_pred_softmax = torch.log_softmax(out, dim = 1)
        top_10 = torch.argsort(y_pred_softmax)[:,-10:] 
        top_n = torch.argsort(y_pred_softmax)[:,-5:]
        _, y_pred_tags = torch.max(y_pred_softmax, dim = 1) 
        print(y_pred_tags)
        tag_ids = screen["screen_tag_id"].to(device)
        print(tag_ids)
        correct_pred = (y_pred_tags == tag_ids).float()
        correct_top = torch.any(top_n == tag_ids.reshape(-1,1) , dim=1)
        correct_top_10 = torch.any(top_10 == tag_ids.reshape(-1,1) , dim=1)
        correct_top_n += correct_top.sum()
        all_correct += correct_pred.sum()
        all_correct_top_10 += correct_top_10.sum()
        all_cnt += len(correct_pred)

print(correct_top_n)
print(all_correct)
print(all_cnt)
print("top-10 acc"+ str(all_correct_top_10/all_cnt))
print("top-5 acc"+ str(correct_top_n/all_cnt))
print("acc: "+ str(all_correct/all_cnt))

torch.save(ui_screen_model.state_dict(), "aliexpress_screen_model")

! cp "aliexpress_screen_model" "/gdrive/My Drive/new_aliexpress_screen_model"
