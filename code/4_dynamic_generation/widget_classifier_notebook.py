# -*- coding: utf-8 -*-
"""widget_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-KldKRthW701IeLRYpmQcoqxunYmatIE
"""

import json
import pandas as pd
from collections import Counter

import torch
import torch.utils.data as data

from google.colab import drive
drive.mount('/gdrive')

def convert_class_to_text_label(widget_type):
    #adopted from Screen2Vec's repo - https://github.com/tobyli/Screen2Vec
    lookup_class_label_dict = {"AdView": 1, "HtmlBannerWebView":1, "AdContainer":1, "ActionBarOverlayLayout": 2, "TabLayout": 2, "TabLayout$SlidingTabStrip": 2,  "TabLayout$TabView": 2, "LinearLayout": 2, "FitWindowsLinearLayout": 2, "CustomScreenLinearLayout": 2, "AppBarLayout": 2, "FrameLayout": 2, "ContentFrameLayout": 2, "FitWindowsFrameLayout": 2, "NoSaveStateFrameLayout": 2, "RelativeLayout": 2, "TableLayout": 2, "BottomTagGroupView": 3, "BottomBar": 3, "ButtonBar": 4, "CardView": 5, "CheckBox": 6, "CheckedTextView":6, "DrawerLayout": 7, "DatePicker": 8, "ImageView": 9, "ImageButton": 10, "GlyphView": 10, "AppCompactButton": 10, "AppCompactImageButton": 10, "ActionMenuItemView":10, "ActionMenuItemPresenter":10, "EditText": 11, "SearchBoxView": 11, "AppCompatAutoCompleteTextView": 11, "TextView": 11, "TextInputLayout": 11,  "ListView": 12, "RecyclerView": 12, "ListPopUpWindow": 12, "tabItem": 12, "GridView": 12, "MapView": 13, "SlidingTab": 14, "NumberPicker": 15, "Switch": 16, "SwitchCompat":16, "ViewPageIndicatorDots": 17, "PageIndicator": 17, "CircleIndicator": 17, "PagerIndicator": 17, "RadioButton": 18, "CheckedTextView": 18, "SeekBar": 19, "Button": 20, "TextView": 20, "ToolBar": 21, "Toolbar": 21, "TitleBar": 21, "ActionBar": 21, "VideoView": 22, "WebView": 23}
    the_class_of_text = widget_type
    if lookup_class_label_dict.get(the_class_of_text):
        return lookup_class_label_dict.get(the_class_of_text)
    else:
        for key, val in lookup_class_label_dict.items():
            if the_class_of_text.endswith(key):
                return val
            if the_class_of_text.startswith(key):
                return val
        return 0

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)

zones = {1: [0, 0, 135, 315], 2: [135, 0, 945, 315], 3: [945, 0, 1080, 315],
             7: [0, 1736, 135, 1920], 8: [135, 1736, 945, 1920], 9: [945, 1736, 1080, 1920],
             4: [0, 315, 135, 1736], 5: [135, 315, 945, 1736], 6: [945, 315, 1080, 1736]}

def convert_bounds_to_screen_zone(bounds):
    x = bounds[0]
    y = bounds[1]
    for zone, zone_bounds in zones.items():
        if zone_bounds[0] <= x <= zone_bounds[2] and zone_bounds[1] <= y <= zone_bounds[3]:
            return zone
    return 0

import json
labels_path = "final_labels_all.csv"

def generate_widget_dict(widget_cntr):
        idx = 0
        w_dict = {}
        for tag in widget_cntr.keys():
            w_dict[idx] = tag
            idx += 1
        return w_dict


widget_df = pd.read_csv(labels_path, usecols=['tag_widget'], na_filter=False)
widget_list = (widget_df['tag_widget'].tolist())
widget_cntr = Counter(widget_list)
widget_dict = generate_widget_dict(widget_cntr)

with open("widget_dict.json", "w") as widget_dict_file:
    widget_dict_file.write(json.dumps(widget_dict))

df = pd.read_csv(labels_path, usecols=['tag_screen'], na_filter=False)

def generate_screen_dict(screen_cntr):
        idx = 0
        s_dict = {}
        for tag in screen_cntr.keys():
            s_dict[tag] = idx
            idx += 1
        return s_dict

screen_list = (df['tag_screen'].tolist())
screen_cntr = Counter(screen_list)
screen_dict = generate_screen_dict(screen_cntr)


with open("screen_dict.json", "w") as screen_dict_file:
    screen_dict_file.write(json.dumps(screen_dict))

from torchvision import transforms
from PIL import Image
import os

class WidgetDataset(data.Dataset):
    def __init__(self, image_path, json_data_path, labels_path):
        self.image_path = image_path
        self.json_data_path = json_data_path
        self.widgets = {}
        df = pd.read_csv(labels_path, usecols=['tag_screen'], na_filter=False)
        self.screen_list = (df['tag_screen'].tolist())
        self.screen_cntr = Counter(self.screen_list)
        self.screen_dict = self.generate_screen_dict()

        self.transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.CenterCrop(min(244, 256)),
            transforms.ToTensor(),
            transforms.Normalize((0.485, 0.456, 0.406),
                                (0.229, 0.224, 0.225))])

        widget_df = pd.read_csv(labels_path, usecols=['tag_widget'], na_filter=False)
        self.widget_list = (widget_df['tag_widget'].tolist())
        self.widget_cntr = Counter(self.widget_list)
        self.widget_dict = self.generate_widget_dict()

        with open(json_data_path, "r") as json_file:
            widgets = json.load(json_file)
            widget_cntr = 0
            for widget in widgets:
                print(widget_cntr)
                id = widget_cntr
                self.widgets[id] = widget
                self.widgets[id]["type_id"] = convert_class_to_text_label(widget["type"])
                self.widgets[id]["screen_tag_id"] = self.screen_dict[widget["screen-tag"]]
                self.widgets[id]["widget_tag_id"] = self.widget_dict[widget["widget-tag"]]

                image = Image.open(os.path.join(self.image_path, widget["image-name"])).convert('RGB')
                image = self.transform(image)
                self.widgets[id]["image"] = image
                self.widgets[id]["location"] = convert_bounds_to_screen_zone(widget["coordinates"])

                widget_cntr += 1

        self.ids = list(self.widgets.keys())

    def generate_screen_dict(self):
        idx = 0
        s_dict = {}
        for tag in self.screen_cntr.keys():
            s_dict[tag] = idx
            idx += 1
        return s_dict

    def generate_widget_dict(self):
        idx = 0
        w_dict = {}
        for tag in self.widget_cntr.keys():
            w_dict[tag] = idx
            idx += 1
        return w_dict

    def __getitem__(self, index):
        widget = self.widgets[index]
        return widget


    def __len__(self):
        return len(self.ids)


def get_loader(image_path, json_path, batch_size, num_workers, labels_path):
    widget = WidgetDataset(image_path, json_path, labels_path)
    data_loader = torch.utils.data.DataLoader(widget, batch_size=batch_size,
                                              num_workers=num_workers)
    return data_loader

import torch
import torch.nn as nn
import torchvision.models as models

class EncoderCNN(nn.Module):
    def __init__(self, hidden_size):
        super(EncoderCNN, self).__init__()
        resnet = models.resnet18(pretrained=True)
        num_ftrs = resnet.fc.in_features
        self.resnet = resnet

        self.hidden_size = hidden_size
        self.linear = nn.Linear(1000, 100)
        self.bn = nn.BatchNorm1d(100, momentum=0.01)

    def forward(self, images):
        with torch.no_grad():
            features = self.resnet(images)
        features = features.reshape(features.size(0), -1)
        features = self.bn(self.linear(features))

        return features

class UIEmbedder(nn.Module):
    def __init__(self, bert_size=768, num_classes=26, class_emb_size=6, num_screens=33, screen_emb_size=7, output_size=71, num_locations=10, location_emb_size=3):
        super().__init__()
        # self.text_embedder = bert
        self.class_embedder = nn.Embedding(num_classes, class_emb_size).to(device)
        self.screen_embedder = nn.Embedding(num_screens, screen_emb_size).to(device)
        self.location_embedder = nn.Embedding(num_locations, location_emb_size).to(device)
        self.bert_size = bert_size
        self.class_size = class_emb_size
        self.screen_size = screen_emb_size
        self.location_size = location_emb_size
        self.image_emb_size = 100
        self.image_encoder = EncoderCNN(self.image_emb_size).to(device)

        self.layer_1 = nn.Linear(self.bert_size+self.class_size+self.screen_size+self.image_emb_size+self.location_size, 512)
        self.layer_2 = nn.Linear(512, 256)
        self.layer_3 = nn.Linear(256, 128)
        self.layer_out = nn.Linear(128, output_size)

        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=0.2)
        self.batchnorm1 = nn.BatchNorm1d(512)
        self.batchnorm2 = nn.BatchNorm1d(256)
        self.batchnorm3 = nn.BatchNorm1d(128)

    def forward(self, text_emb, class_name, screen_name, image, loc):
        # text_emb = torch.as_tensor(self.text_embedder.encode(text))
        class_emb = self.class_embedder(class_name).cuda()
        screen_emb = self.screen_embedder(screen_name).cuda()
        image_emb = self.image_encoder(image).cuda()
        location_emb = self.location_embedder(loc).cuda()
        x = torch.cat((text_emb, class_emb, screen_emb, image_emb, location_emb), 1).cuda()
        #x = torch.cat((text_emb, class_emb, screen_emb, location_emb), 1).cuda()
        # for index in range(len(text)):
        #     if text[index] == '':
        #         x[index] = torch.zeros(self.bert_size + self.class_size + self.screen_size)
        
        o1 = self.layer_1(x).cuda()
        o1 = self.batchnorm1(o1).cuda()
        o1 = self.relu(o1).cuda()

        o2 = self.layer_2(o1).cuda()
        o2 = self.batchnorm2(o2).cuda()
        o2 = self.relu(o2).cuda()
        o2 = self.dropout(o2).cuda()

        o3 = self.layer_3(o2).cuda()
        o3 = self.batchnorm3(o3).cuda()
        o3 = self.relu(o3).cuda()
        o3 = self.dropout(o3).cuda()

        o = self.layer_out(o3).cuda()
        return o

! pip install sentence_transformers


import torch.optim as optim
from sentence_transformers import SentenceTransformer

! cp "geek_widget_model_with_image" "/gdrive/My Drive/geek_widget_model_with_image"

BATCH_SIZE = 100
LEARNING_RATE = 0.0007

app_name = "zappos"
train_file = "/gdrive/My Drive/" +app_name + "_train_data.json"
test_file = "/gdrive/My Drive/" + app_name + "_test_data.json"

# import json
# with open(test_file, "r") as json_file:
#             test_dict = json.load(json_file)
# print(len(test_dict))

train_loader = get_loader("/gdrive/My Drive/widget_images", train_file, BATCH_SIZE, 2, "final_labels_all.csv")
test_loader =  get_loader("/gdrive/My Drive/widget_images", test_file,  BATCH_SIZE, 2, "final_labels_all.csv")

bert = SentenceTransformer('bert-base-nli-mean-tokens').to(device)

ui_model = UIEmbedder().to(device)
criterion = nn.CrossEntropyLoss().to(device)
optimizer = optim.Adam(ui_model.parameters(), lr=LEARNING_RATE)

EPOCHS = 15

for epoch in range(EPOCHS):
  print(epoch)
  batch_cntr = 0
  acc_sum = 0
  for i, widget in enumerate(train_loader):
      text = torch.as_tensor(bert.encode(widget["text"]), device=device)
      class_type = widget["type_id"].to(device)
      screen_tag = widget["screen_tag_id"].to(device)
      image = widget["image"].to(device)
      location = widget["location"].to(device)
      out = ui_model(text, class_type, screen_tag, image,location)
      y_pred = torch.log_softmax(out, dim=1)
      _, y_pred_tags = torch.max(y_pred, dim = 1) 

      tag_ids = widget["widget_tag_id"].to(device)
      correct_pred = (y_pred_tags == tag_ids).float()
      acc = correct_pred.sum() / len(correct_pred)
      acc_sum += acc
      batch_cntr += 1
      loss = criterion(y_pred, tag_ids)
      loss.backward()
      optimizer.step()
      optimizer.zero_grad()
  print("average epoch train acc:"+ str(acc_sum/batch_cntr))

  # all_correct = 0
  # all_cnt = 0
  # with torch.no_grad():
  #     ui_model.eval()
  #     for i, widget in enumerate(test_loader):
  #         text = torch.as_tensor(bert.encode(widget["text"]), device=device)
  #         class_type = widget["type_id"].to(device)
  #         screen_tag = widget["screen_tag_id"].to(device)
  #         image = widget["image"].to(device)
  #         location = widget["location"].to(device)
  #         out = ui_model(text, class_type, screen_tag, image, location)
  #         y_pred_softmax = torch.log_softmax(out, dim = 1)
  #         _, y_pred_tags = torch.max(y_pred_softmax, dim = 1) 
  #         # print(y_pred_tags)
  #         tag_ids = widget["widget_tag_id"].to(device)
  #         correct_pred = (y_pred_tags == tag_ids).float()
  #         # print(widget["widget_tag_id"])
  #         all_correct += correct_pred.sum()
  #         all_cnt += len(correct_pred)


  # print(all_correct)
  # print(all_cnt)
  # print("test acc: "+ str(all_correct/all_cnt))

  # print(loss)

all_correct = 0
all_cnt = 0
correct_top_n = 0
all_correct_top_10 = 0
with torch.no_grad():
    ui_model.eval()
    for i, widget in enumerate(test_loader):
        text = torch.as_tensor(bert.encode(widget["text"]), device=device)
        print(text.shape)
        class_type = widget["type_id"].to(device)
        screen_tag = widget["screen_tag_id"].to(device)
        print(screen_tag)
        print(type(screen_tag))
        print(screen_tag.shape)
        image = widget["image"].to(device)
        print(image.shape)
        location = widget["location"].to(device)
        out = ui_model(text, class_type, screen_tag, image, location)
        y_pred_softmax = torch.log_softmax(out, dim = 1)
        top_10 = torch.argsort(y_pred_softmax)[:,-10:] 
        top_n = torch.argsort(y_pred_softmax)[:,-5:]
        _, y_pred_tags = torch.max(y_pred_softmax, dim = 1) 
        tag_ids = widget["widget_tag_id"].to(device)
        correct_pred = (y_pred_tags == tag_ids).float()
        correct_top = torch.any(top_n == tag_ids.reshape(-1,1) , dim=1)
        correct_top_10 = torch.any(top_10 == tag_ids.reshape(-1,1) , dim=1)
        correct_top_n += correct_top.sum()
        all_correct += correct_pred.sum()
        all_correct_top_10 += correct_top_10.sum()
        all_cnt += len(correct_pred)

print(correct_top_n)
print(all_correct)
print(all_cnt)
print("top-10 acc"+ str(all_correct_top_10/all_cnt))
print("top-n acc"+ str(correct_top_n/all_cnt))
print("acc: "+ str(all_correct/all_cnt))

ids = []
for i, widget in enumerate(test_loader):
  tag_ids = widget["widget_tag_id"].to(device)
  for element in tag_ids:
    ids.append(int(element))
#print(ids)

from collections import Counter
id_counter = Counter(ids)
print(id_counter)

train_ids = []
for i,widget in enumerate(train_loader):
  tag_ids = widget["widget_tag_id"].to(device)
  for element in tag_ids:
      train_ids.append(int(element))
  
train_id_counter = Counter(train_ids)
print(train_id_counter)

torch.save(ui_model.state_dict(), "geek_widget_model_with_image")

loaded_model = UIEmbedder().to(device)

all_correct = 0
all_cnt = 0
correct_top_n = 0
with torch.no_grad():
    loaded_model.eval()
    for i, widget in enumerate(test_loader):
        text = torch.as_tensor(bert.encode(widget["text"]), device=device)
        class_type = widget["type_id"].to(device)
        screen_tag = widget["screen_tag_id"].to(device)
        image = widget["image"].to(device)
        location = widget["location"].to(device)
        out = loaded_model(text, class_type, screen_tag, image, location)
        y_pred_softmax = torch.log_softmax(out, dim = 1)
        top_n = torch.argsort(y_pred_softmax)[:,-5:]
        _, y_pred_tags = torch.max(y_pred_softmax, dim = 1) 
        tag_ids = widget["widget_tag_id"].to(device)
        correct_pred = (y_pred_tags == tag_ids).float()
        correct_top = torch.any(top_n == tag_ids.reshape(-1,1) , dim=1)
        correct_top_n += correct_top.sum()
        all_correct += correct_pred.sum()
        all_cnt += len(correct_pred)

print(correct_top_n)
print(all_correct)
print(all_cnt)
print("top-n acc"+ str(correct_top_n/all_cnt))
print("acc: "+ str(all_correct/all_cnt))

loaded_model.load_state_dict(torch.load("/gdrive/My Drive/usatoday_widget_model_with_image"))

all_correct = 0
all_cnt = 0
correct_top_n = 0
with torch.no_grad():
    loaded_model.eval()
    for i, widget in enumerate(test_loader):
        text = torch.as_tensor(bert.encode(widget["text"]), device=device)
        class_type = widget["type_id"].to(device)
        screen_tag = widget["screen_tag_id"].to(device)
        image = widget["image"].to(device)
        location = widget["location"].to(device)
        out = loaded_model(text, class_type, screen_tag, image, location)
        y_pred_softmax = torch.log_softmax(out, dim = 1)
        top_n = torch.argsort(y_pred_softmax)[:,-5:]
        _, y_pred_tags = torch.max(y_pred_softmax, dim = 1) 
        tag_ids = widget["widget_tag_id"].to(device)
        correct_pred = (y_pred_tags == tag_ids).float()
        correct_top = torch.any(top_n == tag_ids.reshape(-1,1) , dim=1)
        correct_top_n += correct_top.sum()
        all_correct += correct_pred.sum()
        all_cnt += len(correct_pred)

print(correct_top_n)
print(all_correct)
print(all_cnt)
print("top-n acc"+ str(correct_top_n/all_cnt))
print("acc: "+ str(all_correct/all_cnt))

##bbc
BATCH_SIZE = 100
LEARNING_RATE = 0.0007

bbc_train_loader = get_loader("/gdrive/My Drive/widget_images", "bbc_train_data.json", BATCH_SIZE, 2, "final_labels_all.csv")
bbc_test_loader =  get_loader("/gdrive/My Drive/widget_images", "bbc_test_data.json", BATCH_SIZE, 2, "final_labels_all.csv")

bert = SentenceTransformer('bert-base-nli-mean-tokens').to(device)
bbc_ui_model = UIEmbedder().to(device)
criterion = nn.CrossEntropyLoss().to(device)
optimizer = optim.Adam(bbc_ui_model.parameters(), lr=LEARNING_RATE)

EPOCHS = 30

for epoch in range(EPOCHS):
  print(epoch)
  batch_cntr = 0
  acc_sum = 0
  for i, widget in enumerate(bbc_train_loader):
      text = torch.as_tensor(bert.encode(widget["text"]), device=device)
      class_type = widget["type_id"].to(device)
      screen_tag = widget["screen_tag_id"].to(device)
      image = widget["image"].to(device)
      out = bbc_ui_model(text, class_type, screen_tag, image)
      y_pred = torch.softmax(out, dim=1)
      y_pred_softmax = torch.log_softmax(y_pred, dim = 1)
      _, y_pred_tags = torch.max(y_pred_softmax, dim = 1) 

      tag_ids = widget["widget_tag_id"].to(device)
      correct_pred = (y_pred_tags == tag_ids).float()
      acc = correct_pred.sum() / len(correct_pred)
      acc_sum += acc
      batch_cntr += 1
      loss = criterion(y_pred, tag_ids)
      loss.backward()
      optimizer.step()
      optimizer.zero_grad()
  print("average epoch acc:"+ str(acc_sum/batch_cntr))

  print(loss)

all_correct = 0
all_cnt = 0
with torch.no_grad():
    bbc_ui_model.eval()
    for i, widget in enumerate(bbc_test_loader):
        out = bbc_ui_model(widget["text"], widget["type_id"], widget["screen_tag_id"])
        y_pred = torch.softmax(out, dim=1)
        y_pred_softmax = torch.log_softmax(y_pred, dim = 1)
        _, y_pred_tags = torch.max(y_pred_softmax, dim = 1) 
        print(y_pred_tags)
        correct_pred = (y_pred_tags == widget["widget_tag_id"]).float()
        print(widget["widget_tag_id"])
        # acc = correct_pred.sum() / len(correct_pred)
        # print("acc:"+str(acc))
        all_correct += correct_pred.sum()
        all_cnt += len(correct_pred)


print(all_correct)
print(all_cnt)
print("acc: "+ str(all_correct/all_cnt))

all_correct = 0
all_cnt = 0
with torch.no_grad():
    bbc_ui_model.eval()
    for i, widget in enumerate(bbc_test_loader):
        text = torch.as_tensor(bert.encode(widget["text"]), device=device)
        class_type = widget["type_id"].to(device)
        screen_tag = widget["screen_tag_id"].to(device)
        image = widget["image"].to(device)
        out = bbc_ui_model(text, class_type, screen_tag, image)
        y_pred = torch.softmax(out, dim=1)
        y_pred_softmax = torch.log(y_pred)
        # print(y_pred_softmax)
        print(torch.argsort(y_pred_softmax)[:,-5:])
        _, y_pred_tags = torch.max(y_pred_softmax, dim = 1) 
        print(y_pred_tags)
        tag_ids = widget["widget_tag_id"].to(device)
        correct_pred = (y_pred_tags == tag_ids).float()
        print(widget["widget_tag_id"])
        all_correct += correct_pred.sum()
        all_cnt += len(correct_pred)


print(all_correct)
print(all_cnt)
print("acc: "+ str(all_correct/all_cnt))

EPOCHS = 50
BATCH_SIZE = 100
LEARNING_RATE = 0.0007

buzz_train_loader = get_loader("/gdrive/My Drive/widget_images", "buzzfeed_train_data.json", BATCH_SIZE, 5, "final_labels_all.csv")
buzz_test_loader =  get_loader("/gdrive/My Drive/widget_images", "buzzfeed_test_data.json", BATCH_SIZE, 5, "final_labels_all.csv")

bert = SentenceTransformer('bert-base-nli-mean-tokens')

buzz_ui_model = UIEmbedder()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(buzz_ui_model.parameters(), lr=LEARNING_RATE)

for epoch in range(EPOCHS):
  print(epoch)
  batch_cntr = 0
  acc_sum = 0
  for i, widget in enumerate(buzz_train_loader):
      out = buzz_ui_model(widget["text"], widget["type_id"], widget["screen_tag_id"])
      y_pred = torch.softmax(out, dim=1)
      y_pred_softmax = torch.log_softmax(y_pred, dim = 1)
      _, y_pred_tags = torch.max(y_pred_softmax, dim = 1) 
      correct_pred = (y_pred_tags == widget["widget_tag_id"]).float()
      acc = correct_pred.sum() / len(correct_pred)
      acc_sum += acc
      batch_cntr += 1
      loss = criterion(y_pred, widget["widget_tag_id"])
      loss.backward()
      optimizer.step()
      optimizer.zero_grad()
  print("average epoch acc:"+ str(acc_sum/batch_cntr))

  print(loss)

all_correct = 0
all_cnt = 0
with torch.no_grad():
    buzz_ui_model.eval()
    for i, widget in enumerate(buzz_test_loader):
        out = buzz_ui_model(widget["text"], widget["type_id"], widget["screen_tag_id"])
        y_pred = torch.softmax(out, dim=1)
        y_pred_softmax = torch.log_softmax(y_pred, dim = 1)
        _, y_pred_tags = torch.max(y_pred_softmax, dim = 1) 
        print(y_pred_tags)
        correct_pred = (y_pred_tags == widget["widget_tag_id"]).float()
        print(widget["widget_tag_id"])
        # acc = correct_pred.sum() / len(correct_pred)
        # print("acc:"+str(acc))
        all_correct += correct_pred.sum()
        all_cnt += len(correct_pred)


print(all_correct)
print(all_cnt)
print("acc: "+ str(all_correct/all_cnt))

EPOCHS = 30
BATCH_SIZE = 100
LEARNING_RATE = 0.0007

usa_train_loader = get_loader("/gdrive/My Drive/widget_images", "usatoday_train_data.json", BATCH_SIZE, 2, "final_labels_all.csv")
usa_test_loader =  get_loader("/gdrive/My Drive/widget_images", "usatoday_test_data.json", BATCH_SIZE, 2, "final_labels_all.csv")

bert = SentenceTransformer('bert-base-nli-mean-tokens')

usa_ui_model = UIEmbedder(bert)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(usa_ui_model.parameters(), lr=LEARNING_RATE)

for epoch in range(EPOCHS):
  print(epoch)
  batch_cntr = 0
  acc_sum = 0
  for i, widget in enumerate(usa_train_loader):
      out = usa_ui_model(widget["text"], widget["type_id"], widget["screen_tag_id"])
      y_pred = torch.softmax(out, dim=1)
      y_pred_softmax = torch.log_softmax(y_pred, dim = 1)
      _, y_pred_tags = torch.max(y_pred_softmax, dim = 1) 
      correct_pred = (y_pred_tags == widget["widget_tag_id"]).float()
      acc = correct_pred.sum() / len(correct_pred)
      acc_sum += acc
      batch_cntr += 1
      loss = criterion(y_pred, widget["widget_tag_id"])
      loss.backward()
      optimizer.step()
      optimizer.zero_grad()
  print("average epoch acc:"+ str(acc_sum/batch_cntr))

  print(loss)



all_correct = 0
all_cnt = 0
with torch.no_grad():
    usa_ui_model.eval()
    for i, widget in enumerate(usa_test_loader):
        out = usa_ui_model(widget["text"], widget["type_id"], widget["screen_tag_id"])
        y_pred = torch.softmax(out, dim=1)
        y_pred_softmax = torch.log_softmax(y_pred, dim = 1)
        _, y_pred_tags = torch.max(y_pred_softmax, dim = 1) 
        print(y_pred_tags)
        correct_pred = (y_pred_tags == widget["widget_tag_id"]).float()
        print(widget["widget_tag_id"])
        print("------------")
        # acc = correct_pred.sum() / len(correct_pred)
        # print("acc:"+str(acc))
        all_correct += correct_pred.sum()
        all_cnt += len(correct_pred)


print(all_correct)
print(all_cnt)
print("acc: "+ str(all_correct/all_cnt))